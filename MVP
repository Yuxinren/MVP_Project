# import sounddevice as sd
# import numpy as np
# import queue
# import threading
# import whisper
# import your_zephyr_module  # Replace with actual Zephyr module
# import pyttsx3
# # Global variables
# running = True
# audio_queue = queue.Queue()

# # Initialize Whisper model
# whisper_model = whisper.load_model("tiny")  # Choose the appropriate model size

# # Initialize Text-to-Speech engine
# tts_engine = pyttsx3.init()

# def wake_word_detected(audio_data):
#     """
#     Analyze the audio data to detect the wake word.
#     Return True if the wake word is detected, False otherwise.
#     """
#     # Implement wake word detection logic
#     return False

# def audio_callback(indata, frames, time, status):
#     """
#     This is called for each audio buffer from the microphone.
#     """
#     if status:
#         print(status, file=sys.stderr)
#     audio_queue.put(indata.copy())

# def listen_for_wake_word():
#     """
#     Listen to the microphone and check for the wake word.
#     """
#     global running
#     with sd.InputStream(callback=audio_callback):
#         while running:
#             audio_data = audio_queue.get()
#             if wake_word_detected(audio_data):
#                 process_command()

# def process_command():
#     """
#     Capture audio for the command, run it through Whisper, and pass it to Zephyr.
#     """
#     command_audio = capture_command_audio()  # Implement this function
#     transcription = whisper_model.transcribe(command_audio)
#     response = your_zephyr_module.generate_response(transcription)
#     speak_response(response)

# def capture_command_audio():
#     """
#     Capture the command audio. Implement logic to capture audio until a pause is detected.
#     """
#     # Implement audio capture with pause detection
#     return b''  # Return the captured audio bytes

# def speak_response(response_text):
#     """
#     Use the TTS engine to speak out the response.
#     """
#     tts_engine.say(response_text)
#     tts_engine.runAndWait()

# def main():
#     listen_thread = threading.Thread(target=listen_for_wake_word)
#     listen_thread.start()

# if __name__ == "__main__":
#     main()


# import torch
# from transformers import pipeline

# pipe = pipeline("text-generation", model="HuggingFaceH4/zephyr-7b-beta", torch_dtype=torch.bfloat16, device_map="auto")

# messages = [
#     {
#         "role": "system",
#         "content": "You are a friendly chatbot who always responds in the style of a pirate",
#     },
#     {"role": "user", "content": "How many helicopters can a human eat in one sitting?"},
# ]
# prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
# outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)
# print(outputs[0]["generated_text"])

# import torch
# from transformers import pipeline
# model_name = "HuggingFaceH4/zephyr-7b-beta"

# pipe = pipeline(
#     "text-generation",
#     model=model_name,
#     torch_dtype=torch.bfloat16,
#     device_map="auto",
# )

# prompt = "Write a Python function that can clean the HTML tags from the file:"

# outputs = pipe(
#     prompt,
#     max_new_tokens=300,
#     do_sample=True,
#     temperature=0.7,
#     top_k=50,
#     top_p=0.95,
# )
# print(outputs[0]["generated_text"])


# import ollama

# stream = ollama.chat(
#     model='stablelm-zephyr',
#     messages=[
#         {'role': 'user', 
#          'content': 'Act as a smart speaker algorithm to decide which function to call. The speaker received the following user request: "What\\\'s the weather like today in Miami" You need to choose the most appropriate command from the list below to accurately respond to the user\\\'s needs.'},
#         {'role': 'user',
#          'content': 'Now, the user asks: "Set an alarm for 7 AM tomorrow." Choose the appropriate command.'},
#         {'role': 'user',
#          'content': 'Finally, the user requests: "Play some relaxing music." Select the relevant command from your options.'}
#     ],
#     stream=True,
# )

# for chunk in stream:
#     print(chunk['message']['content'], end='', flush=True)


import ollama

def format_user_input(input_text):
    commands = ("Play Music, Set Alarm, Check Weather, Set Timer, Read News, Control Smart Home Devices, "
                "Send a Message, Create Calendar Events, Play Audiobooks or Podcasts, Ask for Definitions or Facts, "
                "Shopping List Additions, Play Radio, Call Uber, Set Reminders, Cooking Assistance, Translation Services, "
                "Flight Information, Stock Market Updates, Find Phone, Bedtime Routine")
    return (f'Act as a smart speaker algorithm to decide which function to call. The speaker received the following user request: "{input_text}" '
            f'You need to choose the most appropriate command from the list below to accurately respond to the user\'s needs. '
            f'Do not answer the question, just pick a function to call. Considering the context and the user\'s intent, '
            f'which command should execute to best fulfill the user\'s request? Select from the options below. Reply with just the command name from the above list and nothing else. '
            f'The available commands are: {commands}')

def process_user_input_and_respond():
    while True:
        user_input = input("Enter your request (or type 'exit' to stop): ")
        if user_input.lower() == 'exit':
            break
        formatted_input = format_user_input(user_input)
        
        # Assuming ollama.chat is the correct method based on your initial example
        stream = ollama.chat(
            model='stablelm-zephyr',
            messages=[{'role': 'user', 'content': formatted_input}],
            stream=True,
        )

        # Assuming the response is directly iterable and provides the desired output
        for chunk in stream:
            print(chunk['message']['content'], end='', flush=True)

process_user_input_and_respond()

